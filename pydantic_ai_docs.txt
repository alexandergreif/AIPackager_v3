TITLE: Extracting Box Dimensions with PydanticAI Agent (Python)
DESCRIPTION: This snippet demonstrates how to configure a PydanticAI `Agent` to extract structured data (a `Box` object) or allow a plain text response if extraction fails. It defines a `Box` Pydantic model and uses `output_type=[Box, str]` to enable both structured and string outputs. The agent is then run synchronously with different inputs to show its behavior.
SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_1

LANGUAGE: Python
CODE:
```
from pydantic import BaseModel

from pydantic_ai import Agent


class Box(BaseModel):
    width: int
    height: int
    depth: int
    units: str


agent = Agent(
    'openai:gpt-4o-mini',
    output_type=[Box, str],
    system_prompt=(
        "Extract me the dimensions of a box, "
        "if you can't extract all data, ask the user to try again."
    ),
)

result = agent.run_sync('The box is 10x20x30')
print(result.output)
#> Please provide the units for the dimensions (e.g., cm, in, m).

result = agent.run_sync('The box is 10x20x30 cm')
print(result.output)
#> width=10 height=20 depth=30 units='cm'
```

----------------------------------------

TITLE: Full Asynchronous Dependency Example in Pydantic-AI Agent (Python)
DESCRIPTION: This comprehensive example illustrates the use of asynchronous dependencies (`httpx.AsyncClient`) across system prompts, tools, and output validators in a Pydantic-AI agent. The `get_system_prompt`, `get_joke_material` (tool), and `validate_output` (output validator) functions are all defined as coroutines, leveraging the asynchronous HTTP client for API interactions. It also shows how to handle retries for invalid output.
SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/dependencies.md#_snippet_3

LANGUAGE: Python
CODE:
```
from dataclasses import dataclass

import httpx

from pydantic_ai import Agent, ModelRetry, RunContext


@dataclass
class MyDeps:
    api_key: str
    http_client: httpx.AsyncClient


agent = Agent(
    'openai:gpt-4o',
    deps_type=MyDeps,
)


@agent.system_prompt
async def get_system_prompt(ctx: RunContext[MyDeps]) -> str:
    response = await ctx.deps.http_client.get('https://example.com')
    response.raise_for_status()
    return f'Prompt: {response.text}'


@agent.tool  # (1)!
async def get_joke_material(ctx: RunContext[MyDeps], subject: str) -> str:
    response = await ctx.deps.http_client.get(
        'https://example.com#jokes',
        params={'subject': subject},
        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},
    )
    response.raise_for_status()
    return response.text


@agent.output_validator  # (2)!
async def validate_output(ctx: RunContext[MyDeps], output: str) -> str:
    response = await ctx.deps.http_client.post(
        'https://example.com#validate',
        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},
        params={'query': output},
    )
    if response.status_code == 400:
        raise ModelRetry(f'invalid response: {response.text}')
    response.raise_for_status()
    return output


async def main():
    async with httpx.AsyncClient() as client:
        deps = MyDeps('foobar', client)
        result = await agent.run('Tell me a joke.', deps=deps)
        print(result.output)
        #> Did you hear about the toothpaste scandal? They called it Colgate.
```

----------------------------------------

TITLE: Initializing a PydanticAI Agent with Gemini 1.5 Flash (Python)
DESCRIPTION: This snippet demonstrates how to initialize a PydanticAI Agent using the 'google-gla:gemini-1.5-flash' model and configure a static system prompt. It then synchronously runs the agent with a user query and prints the LLM's concise output. This example showcases basic agent setup and interaction.
SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/index.md#_snippet_0

LANGUAGE: Python
CODE:
```
from pydantic_ai import Agent

agent = Agent(  # (1)!
    'google-gla:gemini-1.5-flash',
    system_prompt='Be concise, reply with one sentence.',  # (2)!
)

result = agent.run_sync('Where does "hello world" come from?')  # (3)!
print(result.output)
"""
The first known use of "hello, world" was in a 1974 textbook about the C programming language.
"""
```

----------------------------------------

TITLE: Streaming User Profile with Pydantic-AI (Fine-Grained Validation)
DESCRIPTION: This snippet shows how to achieve fine-grained control over validation errors during streaming. It uses `result.stream_structured` to get `ModelResponse` objects and then explicitly calls `result.validate_structured_output` with `allow_partial=not last` to validate the data. A `try-except ValidationError` block is used to catch and handle validation failures, allowing the stream to continue even if intermediate partial data is invalid.
SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/output.md#_snippet_8

LANGUAGE: Python
CODE:
```
from datetime import date

from pydantic import ValidationError
from typing_extensions import TypedDict

from pydantic_ai import Agent


class UserProfile(TypedDict, total=False):
    name: str
    dob: date
    bio: str


agent = Agent('openai:gpt-4o', output_type=UserProfile)


async def main():
    user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'
    async with agent.run_stream(user_input) as result:
        async for message, last in result.stream_structured(debounce_by=0.01):  # (1)!
            try:
                profile = await result.validate_structured_output(  # (2)!
                    message,
                    allow_partial=not last,
                )
            except ValidationError:
                continue
            print(profile)
            #> {'name': 'Ben'}
            #> {'name': 'Ben'}
            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes'}
            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the '}
            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyr'}
            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}
            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}
```

----------------------------------------

TITLE: Installing PydanticAI Slim with Multiple Models and Logfire - Bash
DESCRIPTION: This command demonstrates a slim installation of PydanticAI, including dependencies for multiple models (OpenAI, VertexAI) and the Logfire integration. It allows for a customized installation with only the necessary optional groups.
SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/install.md#_snippet_4

LANGUAGE: bash
CODE:
```
pip/uv-add "pydantic-ai-slim[openai,vertexai,logfire]"
```

----------------------------------------

TITLE: Defining and Running a Simple Pydantic Graph in Python
DESCRIPTION: This snippet demonstrates how to define a simple graph using `pydantic-graph` with two nodes: `DivisibleBy5` and `Increment`. The `DivisibleBy5` node checks if an integer is divisible by 5, ending the graph if true, or transitioning to `Increment` if false. The `Increment` node increments the integer and transitions back to `DivisibleBy5`. The example shows synchronous execution of the graph starting with `DivisibleBy5(4)`.
SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/pydantic_graph/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
from __future__ import annotations

from dataclasses import dataclass

from pydantic_graph import BaseNode, End, Graph, GraphRunContext


@dataclass
class DivisibleBy5(BaseNode[None, None, int]):
    foo: int

    async def run(
        self,
        ctx: GraphRunContext,
    ) -> Increment | End[int]:
        if self.foo % 5 == 0:
            return End(self.foo)
        else:
            return Increment(self.foo)


@dataclass
class Increment(BaseNode):
    foo: int

    async def run(self, ctx: GraphRunContext) -> DivisibleBy5:
        return DivisibleBy5(self.foo + 1)


fives_graph = Graph(nodes=[DivisibleBy5, Increment])
result = fives_graph.run_sync(DivisibleBy5(4))
print(result.output)
```

----------------------------------------

TITLE: Running PydanticAI Agents: Sync, Async, and Streaming
DESCRIPTION: Demonstrates the core methods for executing a PydanticAI agent: `agent.run_sync()` for immediate synchronous results, `agent.run()` for asynchronous execution, and `agent.run_stream()` for processing responses as an asynchronous iterable. This example showcases basic agent initialization and output retrieval.
SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/agents.md#_snippet_2

LANGUAGE: python
CODE:
```
from pydantic_ai import Agent

agent = Agent('openai:gpt-4o')

result_sync = agent.run_sync('What is the capital of Italy?')
print(result_sync.output)
#> Rome


async def main():
    result = await agent.run('What is the capital of France?')
    print(result.output)
    #> Paris

    async with agent.run_stream('What is the capital of the UK?') as response:
        print(await response.get_output())
        #> London
```

----------------------------------------

TITLE: Fixing RuntimeError: This event loop is already running in Jupyter Notebooks
DESCRIPTION: This error is caused by conflicts between the event loops in Jupyter Notebooks (including Google Colab and Marimo) and PydanticAI's. To resolve this, use the `nest-asyncio` library to apply a patch that allows nested event loops, enabling PydanticAI to run correctly within these environments.
SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/troubleshooting.md#_snippet_0

LANGUAGE: python
CODE:
```
import nest_asyncio

nest_asyncio.apply()
```

----------------------------------------

TITLE: Initializing a Basic PydanticAI Agent with System Prompt (Python)
DESCRIPTION: This snippet demonstrates how to initialize a basic PydanticAI agent, specifying the LLM model to use and registering a static system prompt. It shows the synchronous execution of the agent for a simple conversation with the LLM, where the system prompt and user query are sent.
SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/README.md#_snippet_0

LANGUAGE: Python
CODE:
```
from pydantic_ai import Agent

# Define a very simple agent including the model to use, you can also set the model when running the agent.
agent = Agent(
    'google-gla:gemini-1.5-flash',
    # Register a static system prompt using a keyword argument to the agent.
    # For more complex dynamically-generated system prompts, see the example below.
    system_prompt='Be concise, reply with one sentence.',
)

# Run the agent synchronously, conducting a conversation with the LLM.
# Here the exchange should be very short: PydanticAI will send the system prompt and the user query to the LLM,
```

----------------------------------------

TITLE: Agent Delegation Example with PydanticAI in Python
DESCRIPTION: This Python code demonstrates agent delegation using PydanticAI. A 'joke_selection_agent' acts as the parent, delegating joke generation to a 'joke_generation_agent' via the 'joke_factory' tool. It illustrates passing 'ctx.usage' to the delegate agent to aggregate token usage and shows how to retrieve the output from the delegated run.
SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/multi-agent-applications.md#_snippet_0

LANGUAGE: Python
CODE:
```
from pydantic_ai import Agent, RunContext
from pydantic_ai.usage import UsageLimits

joke_selection_agent = Agent(  # (1)!
    'openai:gpt-4o',
    system_prompt=(
        'Use the `joke_factory` to generate some jokes, then choose the best. '
        'You must return just a single joke.'
    ),
)
joke_generation_agent = Agent(  # (2)!
    'google-gla:gemini-1.5-flash', output_type=list[str]
)


@joke_selection_agent.tool
async def joke_factory(ctx: RunContext[None], count: int) -> list[str]:
    r = await joke_generation_agent.run(  # (3)!
        f'Please generate {count} jokes.',
        usage=ctx.usage,  # (4)!
    )
    return r.output  # (5)!


result = joke_selection_agent.run_sync(
    'Tell me a joke.',
    usage_limits=UsageLimits(request_limit=5, total_tokens_limit=300),
)
print(result.output)
#> Did you hear about the toothpaste scandal? They called it Colgate.
print(result.usage())
#> Usage(requests=3, request_tokens=204, response_tokens=24, total_tokens=228)
```

----------------------------------------

TITLE: Instantiating PydanticAI Agent and Models
DESCRIPTION: Demonstrates how to instantiate an Agent by specifying a provider and model name, allowing PydanticAI to automatically select the appropriate model class, provider, and profile. Also describes the option to instantiate model classes directly with explicit provider and profile arguments.
SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/models/index.md#_snippet_1

LANGUAGE: APIDOC
CODE:
```
Agent Instantiation:

1. Automatic Selection:
   - Method: Instantiate Agent with a string formatted as '<provider>:<model>'.
   - Examples:
     Agent("openai:gpt-4o")
     Agent("openrouter:google/gemini-2.5-pro-preview")
   - Outcome: PydanticAI automatically selects the appropriate model class, provider, and profile.

2. Direct Model Class Instantiation:
   - Method: Instantiate a model class directly.
   - Parameters: Pass 'provider' and/or 'profile' arguments explicitly.
```

----------------------------------------

TITLE: Agent Delegation with Shared Dependencies in Pydantic-AI (Python)
DESCRIPTION: This Python example demonstrates how to implement agent delegation in pydantic-ai while effectively managing shared dependencies. It defines a `ClientAndKey` dataclass to encapsulate an `httpx.AsyncClient` and an API key, which are then passed between a `joke_selection_agent` (calling agent) and a `joke_generation_agent` (delegate agent) via the `deps_type` and `RunContext`. This ensures that both agents can reuse the same HTTP client and API key for their respective tool calls, optimizing performance and resource utilization.
SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/docs/multi-agent-applications.md#_snippet_1

LANGUAGE: Python
CODE:
```
from dataclasses import dataclass

import httpx

from pydantic_ai import Agent, RunContext


@dataclass
class ClientAndKey: # (1)!
    http_client: httpx.AsyncClient
    api_key: str


joke_selection_agent = Agent(
    'openai:gpt-4o',
    deps_type=ClientAndKey, # (2)!
    system_prompt=(
        'Use the `joke_factory` tool to generate some jokes on the given subject, '
        'then choose the best. You must return just a single joke.'
    ),
)
joke_generation_agent = Agent(
    'gemini-1.5-flash',
    deps_type=ClientAndKey, # (4)!
    output_type=list[str],
    system_prompt=(
        'Use the \"get_jokes\" tool to get some jokes on the given subject, '
        'then extract each joke into a list.'
    ),
)


@joke_selection_agent.tool
async def joke_factory(ctx: RunContext[ClientAndKey], count: int) -> list[str]:
    r = await joke_generation_agent.run(
        f'Please generate {count} jokes.',
        deps=ctx.deps, # (3)!
        usage=ctx.usage,
    )
    return r.output


@joke_generation_agent.tool # (5)!
async def get_jokes(ctx: RunContext[ClientAndKey], count: int) -> str:
    response = await ctx.deps.http_client.get(
        'https://example.com',
        params={'count': count},
        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},
    )
    response.raise_for_status()
    return response.text


async def main():
    async with httpx.AsyncClient() as client:
        deps = ClientAndKey(client, 'foobar')
        result = await joke_selection_agent.run('Tell me a joke.', deps=deps)
        print(result.output)
        #> Did you hear about the toothpaste scandal? They called it Colgate.
        print(result.usage()) # (6)!
        #> Usage(requests=4, request_tokens=309, response_tokens=32, total_tokens=341)

```

----------------------------------------

TITLE: Building a PydanticAI Bank Support Agent with Tools and DI (Python)
DESCRIPTION: This comprehensive example illustrates how to construct a sophisticated bank support agent using PydanticAI. It showcases defining type-safe dependencies (`SupportDependencies`), structuring agent output (`SupportOutput`), initializing the agent with a specific LLM and system prompt, implementing dynamic system prompts using dependency injection, and registering asynchronous tools (`customer_balance`) that the LLM can call. The `main` function demonstrates how to run the agent asynchronously with injected dependencies, leading to validated and structured responses.
SOURCE: https://github.com/pydantic/pydantic-ai/blob/main/README.md#_snippet_2

LANGUAGE: Python
CODE:
```
from dataclasses import dataclass

from pydantic import BaseModel, Field
from pydantic_ai import Agent, RunContext

from bank_database import DatabaseConn


# SupportDependencies is used to pass data, connections, and logic into the model that will be needed when running
# system prompt and tool functions. Dependency injection provides a type-safe way to customise the behavior of your agents.
@dataclass
class SupportDependencies:
    customer_id: int
    db: DatabaseConn


# This pydantic model defines the structure of the output returned by the agent.
class SupportOutput(BaseModel):
    support_advice: str = Field(description='Advice returned to the customer')
    block_card: bool = Field(description="Whether to block the customer's card")
    risk: int = Field(description='Risk level of query', ge=0, le=10)


# This agent will act as first-tier support in a bank.
# Agents are generic in the type of dependencies they accept and the type of output they return.
# In this case, the support agent has type `Agent[SupportDependencies, SupportOutput]`.
support_agent = Agent(
    'openai:gpt-4o',
    deps_type=SupportDependencies,
    # The response from the agent will, be guaranteed to be a SupportOutput,
    # if validation fails the agent is prompted to try again.
    output_type=SupportOutput,
    system_prompt=(
        'You are a support agent in our bank, give the '
        'customer support and judge the risk level of their query.'
    ),
)


# Dynamic system prompts can make use of dependency injection.
# Dependencies are carried via the `RunContext` argument, which is parameterized with the `deps_type` from above.
# If the type annotation here is wrong, static type checkers will catch it.
@support_agent.system_prompt
async def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:
    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)
    return f"The customer's name is {customer_name!r}"


# `tool` let you register functions which the LLM may call while responding to a user.
# Again, dependencies are carried via `RunContext`, any other arguments become the tool schema passed to the LLM.
# Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.
@support_agent.tool
async def customer_balance(
        ctx: RunContext[SupportDependencies], include_pending: bool
) -> float:
    """Returns the customer's current account balance."""
    # The docstring of a tool is also passed to the LLM as the description of the tool.
    # Parameter descriptions are extracted from the docstring and added to the parameter schema sent to the LLM.
    balance = await ctx.deps.db.customer_balance(
        id=ctx.deps.customer_id,
        include_pending=include_pending,
    )
    return balance




async def main():
    deps = SupportDependencies(customer_id=123, db=DatabaseConn())
    # Run the agent asynchronously, conducting a conversation with the LLM until a final response is reached.
    # Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve an output.
    result = await support_agent.run('What is my balance?', deps=deps)
    # The `result.output` will be validated with Pydantic to guarantee it is a `SupportOutput`. Since the agent is generic,
    # it'll also be typed as a `SupportOutput` to aid with static type checking.
    print(result.output)


    result = await support_agent.run('I just lost my card!', deps=deps)
    print(result.output)
